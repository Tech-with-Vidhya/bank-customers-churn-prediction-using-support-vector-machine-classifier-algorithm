{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fa3cae-6cfa-4977-bd90-12c186c128f9",
   "metadata": {},
   "source": [
    "# BANK CUSTOMER CHURN PREDICTION USING ENSEMBLE - EXTREME GRADIENT BOOSTING CLASSIFIER ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2734b-7195-45c9-8ffa-95505a543e0d",
   "metadata": {},
   "source": [
    "## 1. IMPORTING THE PYTHON LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042f48f4-d2c0-4e7e-89dd-063d0c8f20fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c322feb-d801-4503-b335-f793e81bc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b2392c1-2f79-423f-aaa1-0a1304b3cc50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6475b5845603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#from xgboost.xgbclassifier import XGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import xgboost\n",
    "#from xgboost.xgbclassifier import XGBClassifier\n",
    "from sklearn.tree import export_graphviz, export_text\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "import graphviz\n",
    "import pydotplus\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "print(\"Python Libraries Import Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbfe474-5f39-42ac-aaec-63714ef5d044",
   "metadata": {},
   "source": [
    "## 2. LOADING THE RAW DATA FROM A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad95e2-c19f-472f-8fec-1dc3382fbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_raw_data = pd.read_csv(\"/Users/vidhyalakshmiparthasarathy/.CMVolumes/Google-Drive-pbvidhya/~~~VP_Data_Science/DS_Real_Time_Projects/Bank_Customers_Churn_Prediction_Using_Ensemble_Random_Forest_Classifier_Algorithm/data/Bank_Churn_Raw_Data.csv\")\n",
    "\n",
    "print(\"Raw Data Import Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9564f96-474d-4b8e-b70c-02327aa348d7",
   "metadata": {},
   "source": [
    "## 3. DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b17c3-9b8e-4d25-8d92-d2e0a23c90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the shape of the data\n",
    "\n",
    "actual_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17ef62-cec6-454e-9257-ce8793304776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first 5 Rows of Data Instances\n",
    "\n",
    "actual_raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e4401-cf60-49f7-b534-d69c7e5043d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the last 5 Rows of Data Instances\n",
    "\n",
    "actual_raw_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617b609-e5d9-4be8-810e-83c3f93f82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Column Names in the Raw Data\n",
    "\n",
    "actual_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ddde46-5813-4fe2-8af7-52e5a27028e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Type of the Columns in the Raw Data\n",
    "\n",
    "actual_raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d2747c-7d8d-4cf2-8964-5f1a23f09f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Null Values in the Raw Data\n",
    "\n",
    "actual_raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546dac2d-c7a0-4d1e-82dc-3541bbe88b12",
   "metadata": {},
   "source": [
    "## 4. DATA VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9161f80-4571-4071-b6a0-548f8b762989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a New Data Frame To Include Only the Relevant Input Independent Variables and the Output Dependent Variable\n",
    "\n",
    "raw_data = actual_raw_data[['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',\n",
    "                           'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
    "                           'EstimatedSalary', 'Exited']]\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff937af-4f5a-4d63-adb0-9e96d30f3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair Plot - Visualising the Relationship Between The Variables\n",
    "\n",
    "raw_data_graph = sns.pairplot(raw_data, hue='Exited', diag_kws={'bw_method':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43e1ba6-de9a-4b45-ab00-b41e1ef1b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Plot - Visualising the Relationship Between Each OF The Input Independent Variables and the Output Dependent Variable\n",
    "\n",
    "input_features = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',\n",
    "                 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
    "                 'EstimatedSalary']\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_count_plot = sns.countplot(x=feature, data=raw_data, hue='Exited', palette=\"Set3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2874787-a41e-4488-9bd6-c3f1f559b35a",
   "metadata": {},
   "source": [
    "### Scatter Plot - Visualising the Relationship Between Each OF The Input Independent Variables and the Output Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1543ad-8f0b-4710-a2c0-a4bf48d843e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot - Visualising the Relationship Between Each OF The Input Independent Variables and the Output Dependent Variable\n",
    "\n",
    "input_features = ['CreditScore', 'Geography', 'Gender', 'Age', 'Tenure',\n",
    "                 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n",
    "                 'EstimatedSalary']\n",
    "\n",
    "# Input Variable 'CreditScore'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_1 = sns.scatterplot(data=raw_data, x='CreditScore', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49506af-74aa-421d-9c65-3ed61b09e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'Geography'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_2 = sns.scatterplot(data=raw_data, x='Geography', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4529c-534f-4ccf-8ad6-22817e26d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'Gender'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_3 = sns.scatterplot(data=raw_data, x='Gender', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1ebdf-7269-40af-8572-ba39b6ca4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'Age'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure() \n",
    "    feature_scatter_plot_4 = sns.scatterplot(data=raw_data, x='Age', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778cdfa6-3767-4f63-b58a-289331ef5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'Tenure'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_5 = sns.scatterplot(data=raw_data, x='Tenure', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d91a98-cbaa-4796-8d02-84d7841140a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'Balance'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_6 = sns.scatterplot(data=raw_data, x='Balance', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b437f57-b9df-4aa5-9028-145db8f6b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'NumOfProducts'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_7 = sns.scatterplot(data=raw_data, x='NumOfProducts', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b65bc-5cc8-41d8-b1b6-25a8934e277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'HasCrCard'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_8 = sns.scatterplot(data=raw_data, x='HasCrCard', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec593cd-9724-4e45-b51f-1820b5ae4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'IsActiveMember'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_9 = sns.scatterplot(data=raw_data, x='IsActiveMember', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7549b-e5a6-4745-b498-5fa6c22f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Variable 'EstimatedSalary'\n",
    "\n",
    "for feature in input_features:\n",
    "    plt.figure()\n",
    "    feature_scatter_plot_10 = sns.scatterplot(data=raw_data, x='EstimatedSalary', y=feature, hue='Exited')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593f7dd-7dee-4c6e-b6be-712fab813c13",
   "metadata": {},
   "source": [
    "## 5. DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d44cae-f42c-4b13-8ff4-814754a79399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Categorical Variables into Numeric One-Hot Encoded Variables for Decision Tree IDE Model Training Purposes\n",
    "\n",
    "raw_data_pp = pd.get_dummies(raw_data, columns=['Geography', 'Gender', 'HasCrCard', 'IsActiveMember'])\n",
    "\n",
    "print(\"Execution Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6532b24-2914-422d-aedf-35c71db7e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Columns of the Pre-processed Raw Data Frame after Applying One-Hot Encoding Method\n",
    "\n",
    "raw_data_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36315a-65fe-4eea-a329-29a493396c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Shape of the Pre-processed Raw Data Frame after Applying One-Hot Encoding Method\n",
    "\n",
    "raw_data_pp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255e734-8316-4c85-86ed-079345c2603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the Continuous Variables Columns to Scale to a Value Between 0 and 1 for Decision Tree IDE Model Training Purposes\n",
    "\n",
    "norm_scale_features = ['CreditScore', 'Age', 'Balance','EstimatedSalary']\n",
    "\n",
    "norm_scale = MinMaxScaler()\n",
    "\n",
    "raw_data_pp[norm_scale_features] = norm_scale.fit_transform(raw_data_pp[norm_scale_features])\n",
    "\n",
    "print(\"Scaling is Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba8eb9-61d6-486b-a00a-0f1af9ff03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying all the Columns of the Final Pre-processed Raw Data Frame after Applying the Scaling Method\n",
    "\n",
    "raw_data_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0cd4ed-7505-4dac-992e-91950120812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Shape of the Pre-processed Raw Data Frame after Applying the Scaling Method\n",
    "\n",
    "raw_data_pp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed2626-26e3-4ca3-848e-eecb9c254bb0",
   "metadata": {},
   "source": [
    "## 6. DATA SPLIT AS TRAIN DATA AND VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5e3b9-eb51-4d36-95e4-f02873f710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Input and the Target Vectors for Decision Tree IDE Model Training Purposes\n",
    "\n",
    "# Input (Independent) Features/Attributes\n",
    "X = raw_data_pp.drop('Exited', axis=1).values\n",
    "\n",
    "# Output (Dependent) Target Attribute\n",
    "y = raw_data_pp['Exited'].values\n",
    "\n",
    "print(\"Execution Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d873d1-02f6-4205-8d00-2f4226dc3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Shape of the Input and the Output Vectors\n",
    "\n",
    "print(\"The Input Vector Shape is {}\".format(X.shape))\n",
    "print(\"The Output Vector Shape is {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784da3f2-6814-4408-a790-2e18ffae3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data Between Train and Validation Data\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=1)\n",
    "\n",
    "print(\"Execution Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f82cb-6c9b-4172-9e39-123d34de33b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying the Shape of the Train and the Validation Data\n",
    "\n",
    "print(\"Input Train: {}\".format(X_train.shape))\n",
    "print(\"Output Train: {}\\n\".format(y_train.shape))\n",
    "print(\"Input Validation: {}\".format(X_validate.shape))\n",
    "print(\"Output Validation: {}\".format(y_validate.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0dd6c6-dbd4-40e1-adec-fba4f86231c8",
   "metadata": {},
   "source": [
    "## 7. TRAINING THE RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781d154-008a-4cac-8b10-0e41cbec6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Instance of the Random Forest Classifier Model with the Default Parameter Values\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, criterion='entropy', \n",
    "                                             max_depth=None, min_samples_split=2, \n",
    "                                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                             max_features='auto', max_leaf_nodes=None, \n",
    "                                             min_impurity_decrease=0.0, bootstrap=True, \n",
    "                                             oob_score=False, n_jobs=None, random_state=10, \n",
    "                                             verbose=0, warm_start=False, class_weight=None, \n",
    "                                             ccp_alpha=0.0, max_samples=None)\n",
    "\n",
    "print(\"Model Training Started.....\")\n",
    "\n",
    "# Training the Random Forest Classifier Model using ID3 Algorithm\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model Training Completed.....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08f36d-f49a-4468-a674-a9f6dcdbba6d",
   "metadata": {},
   "source": [
    "## 8. RANDOM FOREST GRAPHICAL REPRESENTATION AND VISUALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f62ca-1518-4026-881a-f5c4790b2ce5",
   "metadata": {},
   "source": [
    "### Method 1 : Visualising the Random Forest Base Estimators Decision Trees using export_graphviz() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9804b6-32f6-4fd3-8ad2-8bd422fe0f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 : Visualising the Random Forest Using export_graphviz() Function\n",
    "\n",
    "# Plotting the 0th Index (First Decision Tree) in the Base Decision Tree Estimator\n",
    "\n",
    "# Defining the Random Forest Graph Data\n",
    "graph_data = tree.export_graphviz(random_forest_model.estimators_[0], out_file=None, \n",
    "                                  feature_names=raw_data_pp.drop('Exited', axis=1).columns,\n",
    "                                  class_names=raw_data_pp['Exited'].unique().astype(str),\n",
    "                                  filled=True, rounded=True, special_characters=True,\n",
    "                                  impurity=True)\n",
    "\n",
    "#graph_data\n",
    "\n",
    "# Creating the Random Forest for the Above Graph Data using Graphviz\n",
    "random_forest_graph = graphviz.Source(graph_data)\n",
    "\n",
    "# Visualising the Decision Tree\n",
    "random_forest_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4558ed85-3e2a-4f83-b4bb-49cc17ced030",
   "metadata": {},
   "source": [
    "### Method 2 : Visualising the Random Forest Base Estimators Decision Trees using graph_from_dot_data() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3292f-935c-466b-9ea5-2bedb8c50c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 : Visualising the Random Forest Using graph_from_dot_data() Function\n",
    "\n",
    "# Plotting the 1st Index (Second Decision Tree) in the Base Decision Tree Estimator\n",
    "\n",
    "# Defining the Random Forest Graph Data\n",
    "graph_data = tree.export_graphviz(random_forest_model.estimators_[1], out_file=None, \n",
    "                                  feature_names=raw_data_pp.drop('Exited', axis=1).columns,\n",
    "                                  class_names=raw_data_pp['Exited'].unique().astype(str),\n",
    "                                  filled=True, rounded=True, special_characters=True,\n",
    "                                  impurity=True)\n",
    "\n",
    "#graph_data\n",
    "\n",
    "\n",
    "# Creating the Random Forest for the Above Graph Data using pydotplus\n",
    "pydot_graph = pydotplus.graph_from_dot_data(graph_data)\n",
    "pydot_graph.write_png('Original_Random_Forest.png')\n",
    "pydot_graph.set_size('\"8,8!\"')\n",
    "pydot_graph.write_png('Resized_Random_Forest.png')\n",
    "\n",
    "pydot_graph\n",
    "\n",
    "print(\"Execution Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc2c42-d1af-469a-b667-656a43017774",
   "metadata": {},
   "source": [
    "### Method 3 : Visualising the Random Forest Base Estimators Decision Trees using plot_tree() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880b08-3d86-43d6-a21f-1addcad67a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 : Visualising the Random Forest Using plot_tree() Function\n",
    "\n",
    "# Plotting the 99th Index (100th Decision Tree) in the Base Decision Tree Estimator\n",
    "\n",
    "# Defining the Random Forest Graph Data\n",
    "random_forest_graph = tree.plot_tree(random_forest_model.estimators_[99], feature_names=raw_data_pp.drop('Exited', axis=1).columns,\n",
    "                                     class_names=raw_data_pp['Exited'].unique().astype(str),\n",
    "                                     filled=True, rounded=True, fontsize=8)\n",
    "\n",
    "# Visualising the Random Forest\n",
    "random_forest_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f86db-2b99-494c-861f-c350fb1697fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = raw_data_pp.drop('Exited', axis=1).columns\n",
    "feature_names = []\n",
    "for feature in features:\n",
    "    feature_names.append(feature)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9c65d-940c-4d28-8fda-b443b8a1d3f8",
   "metadata": {},
   "source": [
    "### Method 4 : Visualising the Random Forest Base Estimators Decision Trees in Text Format using export_text() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93d067-4d2e-4c50-8081-8c4c76fc6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4 : Visualising the Random Forest in Text Format using export_text() Function\n",
    "\n",
    "# Plotting the 49th Index (50th Decision Tree) in the Base Decision Tree Estimator\n",
    "\n",
    "# Creating a List of Input Feature Names\n",
    "features = raw_data_pp.drop('Exited', axis=1).columns\n",
    "feature_names_list = []\n",
    "for feature in features:\n",
    "    feature_names_list.append(feature)\n",
    "\n",
    "# Defining the Random Forest Textual Representation Data\n",
    "random_forest_text = tree.export_text(random_forest_model.estimators_[49], feature_names=feature_names_list,\n",
    "                                      spacing=4)\n",
    "\n",
    "# Visualising the Random Forest in the Textual Format\n",
    "print(random_forest_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf78c25-f75b-4dd5-a88f-d6c5f2bffb0a",
   "metadata": {},
   "source": [
    "## 9. RETRIEVING THE FEATURE IMPORTANCE VALUES OF THE INPUT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8805c-6a77-4990-b3dc-87a13e825ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the Information Gain i.e.; Feature Importance Values of the Input Features\n",
    "\n",
    "# Creating an Empty Data Frame to Hold the Feature Name and the Feature's Importance Values\n",
    "ig_df_final = pd.DataFrame()\n",
    "\n",
    "# Looping Through Each and Every Input Feature and Retrieving the Feature Importance Value for Each Feature\n",
    "for feature, column in enumerate(raw_data_pp.drop('Exited', axis=1)):\n",
    "    print(\"{} - {}\".format(column, random_forest_model.feature_importances_[feature]))\n",
    "    \n",
    "    # Creating a Data Frame to Include the Feature Name and the Corresponding Feature Importance Value\n",
    "    ig_df = pd.DataFrame({'Feature': [column], 'Feature Importance': [random_forest_model.feature_importances_[feature]]})\n",
    "    \n",
    "    # Concatenating the Individual Feature Data Frame with the Final Data Frame\n",
    "    ig_df_final = pd.concat([ig_df_final, ig_df], axis=0, ignore_index=True)\n",
    "    \n",
    "# Ordering the Feature Importance Values in the Increasing Order of Importance\n",
    "ig_df_final_sorted = ig_df_final.sort_values(by='Feature Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "ig_df_final_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086e0194-2e3e-43b6-b5d1-a1751e587b27",
   "metadata": {},
   "source": [
    "## 10. CALCULATING AND COMPARING THE TRAINING AND VALIDATION ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60ba11-e95b-4b74-ba94-c7318365071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on the Train Data\n",
    "print(\"Training Accuracy: \", random_forest_model.score(X_train, y_train))\n",
    "\n",
    "# Accuracy on the Validation Data\n",
    "print(\"Validation Accuracy: \", random_forest_model.score(X_validate, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6d592-9ff7-4262-8b8b-1515ee03b188",
   "metadata": {},
   "source": [
    "## 11. VALIDATING THE CLASSIFIER RESULTS ON THE VALIDATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7b7a3-eac5-45b0-a3c1-7e610d67fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the Classifier Results on the Validation Data\n",
    "\n",
    "y_validate_pred = random_forest_model.predict(X_validate)\n",
    "\n",
    "y_validate_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2c855-1dfb-410f-9af6-3a6ad67dcd06",
   "metadata": {},
   "source": [
    "## 12. COMPARING THE VALIDATION ACTUALS WITH THE VALIDATION PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dbfd9e-c391-441d-9d70-29dc0a5ecd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the Validation Predictions with the Validation Actuals for the first 20 Data Instances\n",
    "\n",
    "# Validation Actuals\n",
    "print(y_validate[:20])\n",
    "\n",
    "# Validation Predictions\n",
    "print(y_validate_pred[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba88f92-50bc-453e-bc1b-87a965b3d6dd",
   "metadata": {},
   "source": [
    "## 13. CONFUSION MATRIX BETWEEN THE VALIDATION ACTUALS AND THE VALIDATION PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e7357-07e9-4157-baba-2432d9176018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Instance of Confusion Matrix\n",
    "cm_validation_matrix = confusion_matrix(y_validate, y_validate_pred)\n",
    "\n",
    "print(\"Execution Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89da77d-1ce5-40b8-87b8-0174268eee99",
   "metadata": {},
   "source": [
    "## Method 1 : Plotting the Confusion Matrix with Numeric Values using Seaborn heatmap() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc2615-6933-4f51-89d3-8b5ded455080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 : Plotting the Confusion Matrix with Numeric Values using Seaborn heatmap() Function\n",
    "\n",
    "churn_cm_plot_1 = sns.heatmap(cm_validation_matrix, annot=True)\n",
    "churn_cm_plot_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c8dad-4e83-415d-9fab-679828b46e9a",
   "metadata": {},
   "source": [
    "## Method 2 : Plotting the Confusion Matrix with Percentage Values using Seaborn heatmap() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91438c6-3add-4a7c-ab3b-49bd195b8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2 : Plotting the Confusion Matrix with Percentage Values Rounded-off to 2 Decimal Places using Seaborn heatmap() Function\n",
    "\n",
    "churn_cm_plot_2 = sns.heatmap(cm_validation_matrix/np.sum(cm_validation_matrix), annot=True, fmt='0.2%', cmap='plasma')\n",
    "churn_cm_plot_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe2ab34-a9ec-47cc-9cf7-36da418a3a05",
   "metadata": {},
   "source": [
    "## Method 3 : Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76edd9f4-aab0-484d-a8ce-82ec022a32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 : Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function\n",
    "\n",
    "cm_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "\n",
    "cm_counts = [\"{0:0.0f}\".format(value) for value in cm_validation_matrix.flatten()]\n",
    "\n",
    "cm_percentages = [\"{0:0.2%}\".format(value) for value in cm_validation_matrix.flatten()/np.sum(cm_validation_matrix)]\n",
    "\n",
    "cm_labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cm_names,cm_counts,cm_percentages)]\n",
    "\n",
    "cm_labels = np.asarray(cm_labels).reshape(2,2)\n",
    "\n",
    "sns.heatmap(cm_validation_matrix, annot=cm_labels, fmt='', cmap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d080c-3349-4fec-9b20-3728c13cefa4",
   "metadata": {},
   "source": [
    "## 14. CLASSIFICATION REPORT BETWEEN THE VALIDATION ACTUALS AND THE VALIDATION PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc5663-8ebf-4d90-b28c-2e13f53762c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report and Metrics between the Validation Actuals and the Validation Predictions\n",
    "\n",
    "target_names = ['No Churn', 'Churn']\n",
    "\n",
    "# Defining the Classification Report for the Validation Data\n",
    "classification_report_validation = classification_report(y_validate, y_validate_pred, target_names=target_names)\n",
    "\n",
    "# Displaying the Classification Report\n",
    "print(classification_report_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b340f-90af-443c-90b9-5cdccac5577b",
   "metadata": {},
   "source": [
    "## 15. INDIVIDUAL CLASSIFIER METRICS BETWEEN THE VALIDATION ACTUALS AND THE VALIDATION PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ae826-b06b-4128-8295-1f2a41022fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Classifier Metrics between the Validation Actuals and the Validation Predictions\n",
    "\n",
    "# Accuracy\n",
    "churn_accuracy = round((accuracy_score(y_validate, y_validate_pred))*100, 2)\n",
    "\n",
    "# F1-score\n",
    "churn_f1_score = round((f1_score(y_validate, y_validate_pred)*100), 2)\n",
    "\n",
    "# Precision\n",
    "churn_precision = round((precision_score(y_validate, y_validate_pred)*100), 2)\n",
    "\n",
    "# Recall\n",
    "churn_recall = round((recall_score(y_validate, y_validate_pred)*100), 2)\n",
    "\n",
    "# ROC AUC Score\n",
    "churn_roc_auc_score = round((roc_auc_score(y_validate, y_validate_pred)*100), 2)\n",
    "\n",
    "print(\"Customer Churn Classifier - Accuracy: {}%\".format(churn_accuracy))\n",
    "print(\"Customer Churn Classifier - F1-Score: {}%\".format(churn_f1_score))\n",
    "print(\"Customer Churn Classifier - Precision: {}%\".format(churn_precision))\n",
    "print(\"Customer Churn Classifier - Recall: {}%\".format(churn_recall))\n",
    "print(\"Customer Churn Classifier - ROC AUC Score: {}%\".format(churn_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b07ef-687e-4f7f-862e-c80a59aba934",
   "metadata": {},
   "source": [
    "## 16. TUNING THE HYPER-PARAMETERS OF THE RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135292e-aa82-4010-acb1-364b689f7ed3",
   "metadata": {},
   "source": [
    "### Creating a For Loop to Tune the Random Forest Classifier for the Various Combinations of the Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2326e-a0cf-427a-80fe-a53faf06014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the max_features values\n",
    "\n",
    "max_features_full = ig_df_final_sorted.shape[0]\n",
    "max_feature_sqrt = math.sqrt(ig_df_final_sorted.shape[0])\n",
    "max_feature_log2 = math.log2(ig_df_final_sorted.shape[0])\n",
    "\n",
    "print(max_features_full)\n",
    "print(max_feature_sqrt)\n",
    "print(max_feature_log2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40adcf9-ffcd-4a27-bb3b-2e32fea7d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Method 1\n",
    "\n",
    "# Creating a For Loop to Tune the Random Forest Classifier for the Various Combinations of the Hyper-Parameters\n",
    "\n",
    "# Setting the Values of the Hyper-Parameters to be used for Tuning the Random Forest Classifier\n",
    "n_estimators = 100\n",
    "criterion = 'entropy'\n",
    "max_depth = [1, 2, 3, 4, 5, 6]                    #p1\n",
    "min_samples_split = [2, 3, 4, 5]                  #p2             \n",
    "min_samples_leaf = [1, 2, 3, 4, 5]                #p3                         \n",
    "min_weight_fraction_leaf = [0.0, 0.25, 0.5]       #p4\n",
    "max_features = ['auto', 'sqrt', 'log2']           #p5                    \n",
    "max_leaf_nodes = [None, 1, 2]                     #p6                   \n",
    "min_impurity_decrease = [0.0, 0.1, 0.2, 0.3]      #p7\n",
    "bootstrap = True                                   \n",
    "oob_score = False                                   \n",
    "n_jobs = [1, 2, -1]                               #p8                                \n",
    "random_state = [None, 10]                         #p9         \n",
    "verbose = [0, 1, 2]                               #p10                                         \n",
    "warm_start= False                         \n",
    "class_weight = None                                \n",
    "ccp_alpha = [0.0, 0.1, 0.2, 0.3]                  #p11                      \n",
    "max_samples = None          \n",
    "\n",
    "for p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11 in product(max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, \n",
    "                                                            max_features, max_leaf_nodes, min_impurity_decrease, n_jobs, \n",
    "                                                            random_state, verbose, ccp_alpha):\n",
    "    \n",
    "    # Defining the Random Forest Classifier Model with its Hyper-Parameters\n",
    "    random_forest_tune_model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, \n",
    "                                                     max_depth=p1, min_samples_split=p2, \n",
    "                                                     min_samples_leaf=p3, \n",
    "                                                     min_weight_fraction_leaf=p4, \n",
    "                                                     max_features=p5, max_leaf_nodes=p6, \n",
    "                                                     min_impurity_decrease=p7, bootstrap=bootstrap, \n",
    "                                                     oob_score=oob_score, n_jobs=p8, random_state=p9, \n",
    "                                                     verbose=p10, warm_start=warm_start, class_weight=class_weight, \n",
    "                                                     ccp_alpha=p11)\n",
    "    \n",
    "    # Fitting and Training the Random Forest Classifier Model based on its Hyper-Parameters\n",
    "    random_forest_tune_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Classifier on the Validation Data\n",
    "    y_validate_tune_pred = random_forest_tune_model.predict(X_validate)\n",
    "    \n",
    "    # Calculating the Accuracy\n",
    "    churn_tune_accuracy = round((accuracy_score(y_validate, y_validate_tune_pred))*100, 2)\n",
    "    \n",
    "    # Displaying the Accuracy Metrics for the Various Combinations of the Hyper-Parameters Tuning\n",
    "    print(\"max_depth: {}, min_samples_split: {}, min_samples_leaf: {}, min_weight_fraction_leaf: {}, max_features: {}, max_leaf_nodes: {}, min_impurity_decrease: {}, n_jobs: {}, random_state: {}, verbose: {}, ccp_alpha: {} \\n Classification Accuracy: {}% \\n\".format(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, churn_tune_accuracy))\n",
    "    \n",
    "    # Defining the Instance of Confusion Matrix\n",
    "    cm_validation_matrix_tune = confusion_matrix(y_validate, y_validate_tune_pred)\n",
    "                                                                                                                                    \n",
    "    # Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function\n",
    "    cm_names_tune = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "    cm_counts_tune = [\"{0:0.0f}\".format(value) for value in cm_validation_matrix_tune.flatten()]\n",
    "    cm_percentages_tune = [\"{0:0.2%}\".format(value) for value in cm_validation_matrix_tune.flatten()/np.sum(cm_validation_matrix_tune)]\n",
    "    cm_labels_tune = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cm_names_tune, cm_counts_tune, cm_percentages_tune)]\n",
    "    cm_labels_tune = np.asarray(cm_labels_tune).reshape(2,2)\n",
    "    sns.heatmap(cm_validation_matrix_tune, annot=cm_labels_tune, fmt='', cmap='jet')                                                                                            \n",
    "    title = \"Confusion Matrix - max_depth: {}, min_samples_split: {}, min_samples_leaf: {}, min_weight_fraction_leaf: {}, max_features: {}, max_leaf_nodes: {}, min_impurity_decrease: {}, n_jobs: {}, random_state: {}, verbose: {}, ccp_alpha: {} \\n Classification Accuracy: {}%\".format(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11, churn_tune_accuracy)\n",
    "                                                                \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967bf8ce-a373-43b1-830e-4d42936ca33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "\n",
    "# Creating a For Loop to Tune the Random Forest Classifier for the Various Combinations of the Hyper-Parameters\n",
    "\n",
    "# Setting the Values of the Hyper-Parameters to be used for Tuning the Random Forest Classifier\n",
    "n_estimators = 100\n",
    "criterion = 'entropy'\n",
    "max_depth = [1, 2, 3, 4, 5, 6]                    #p1\n",
    "min_samples_split = 2                            \n",
    "min_samples_leaf = 1                                       \n",
    "min_weight_fraction_leaf = 0.0      \n",
    "max_features = ['auto', 'sqrt', 'log2']           #p2                   \n",
    "max_leaf_nodes = None                                      \n",
    "min_impurity_decrease = 0.0     \n",
    "bootstrap = True                                   \n",
    "oob_score = False                                   \n",
    "n_jobs = None                                                            \n",
    "random_state = 10                                 \n",
    "verbose = 0                                                                \n",
    "warm_start= False                         \n",
    "class_weight = None                                \n",
    "ccp_alpha = 0.0                                   \n",
    "max_samples = None          \n",
    "\n",
    "scenario_id = 0\n",
    "\n",
    "for p1, p2 in product(max_depth, max_features):\n",
    "    \n",
    "    # Defining the Random Forest Classifier Model with its Hyper-Parameters\n",
    "    random_forest_tune_model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, \n",
    "                                                     max_depth=p1, min_samples_split=min_samples_split, \n",
    "                                                     min_samples_leaf=min_samples_leaf, \n",
    "                                                     min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "                                                     max_features=p2, max_leaf_nodes=max_leaf_nodes, \n",
    "                                                     min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap, \n",
    "                                                     oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n",
    "                                                     verbose=verbose, warm_start=warm_start, class_weight=class_weight, \n",
    "                                                     ccp_alpha=ccp_alpha)\n",
    "    \n",
    "    # Fitting and Training the Random Forest Classifier Model based on its Hyper-Parameters\n",
    "    random_forest_tune_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Classifier on the Validation Data\n",
    "    y_validate_tune_pred = random_forest_tune_model.predict(X_validate)\n",
    "    \n",
    "    # Calculating the Accuracy\n",
    "    churn_tune_accuracy = round((accuracy_score(y_validate, y_validate_tune_pred))*100, 2)\n",
    "    \n",
    "    # Incrementing the Scenario_ID for Tracking\n",
    "    scenario_id += 1\n",
    "    \n",
    "    # Displaying the Accuracy Metrics for the Various Combinations of the Hyper-Parameters Tuning\n",
    "    print(\" Scenario {} - max_depth: {}, max_features: {}, Classification Accuracy: {}%\".format(scenario_id, p1, p2, churn_tune_accuracy))\n",
    "    \n",
    "    # Defining the Instance of Confusion Matrix\n",
    "    cm_validation_matrix_tune = confusion_matrix(y_validate, y_validate_tune_pred)\n",
    "                                                                                                                                    \n",
    "    # Plotting the Confusion Matrix with Numeric Values, Percentage Values and the Corresponding Text using Seaborn heatmap() Function\n",
    "    cm_names_tune = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "    cm_counts_tune = [\"{0:0.0f}\".format(value) for value in cm_validation_matrix_tune.flatten()]\n",
    "    cm_percentages_tune = [\"{0:0.2%}\".format(value) for value in cm_validation_matrix_tune.flatten()/np.sum(cm_validation_matrix_tune)]\n",
    "    cm_labels_tune = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(cm_names_tune, cm_counts_tune, cm_percentages_tune)]\n",
    "    cm_labels_tune = np.asarray(cm_labels_tune).reshape(2,2)\n",
    "    plot = sns.heatmap(cm_validation_matrix_tune, annot=cm_labels_tune, fmt='', cmap='jet')                                                                                           \n",
    "    title = \"Confusion Matrix {} - max_depth: {}, max_features: {}, Classification Accuracy: {}%\".format(scenario_id, p1, p2, churn_tune_accuracy)\n",
    "    plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76099d-0891-4585-a863-c433765b2732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3\n",
    "\n",
    "# Creating a For Loop to Tune the Random Forest Classifier for the Various Combinations of the Hyper-Parameters\n",
    "\n",
    "# Setting the Values of the Hyper-Parameters to be used for Tuning the Random Forest Classifier\n",
    "n_estimators = 100\n",
    "criterion = 'entropy'\n",
    "max_depth = [1, 2, 3, 4, 5, 6]                    #p1\n",
    "min_samples_split = 2                            \n",
    "min_samples_leaf = 1                                       \n",
    "min_weight_fraction_leaf = 0.0      \n",
    "max_features = ['auto', 'sqrt', 'log2']           #p2                   \n",
    "max_leaf_nodes = None                                      \n",
    "min_impurity_decrease = 0.0     \n",
    "bootstrap = True                                   \n",
    "oob_score = False                                   \n",
    "n_jobs = None                                                            \n",
    "random_state = 10                                 \n",
    "verbose = 0                                                                \n",
    "warm_start= False                         \n",
    "class_weight = None                                \n",
    "ccp_alpha = 0.0                                   \n",
    "max_samples = None          \n",
    "\n",
    "scenario_id = 0\n",
    "\n",
    "for p1, p2 in product(max_depth, max_features):\n",
    "    \n",
    "    # Defining the Random Forest Classifier Model with its Hyper-Parameters\n",
    "    random_forest_tune_model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, \n",
    "                                                     max_depth=p1, min_samples_split=min_samples_split, \n",
    "                                                     min_samples_leaf=min_samples_leaf, \n",
    "                                                     min_weight_fraction_leaf=min_weight_fraction_leaf, \n",
    "                                                     max_features=p2, max_leaf_nodes=max_leaf_nodes, \n",
    "                                                     min_impurity_decrease=min_impurity_decrease, bootstrap=bootstrap, \n",
    "                                                     oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n",
    "                                                     verbose=verbose, warm_start=warm_start, class_weight=class_weight, \n",
    "                                                     ccp_alpha=ccp_alpha)\n",
    "    \n",
    "    # Fitting and Training the Random Forest Classifier Model based on its Hyper-Parameters\n",
    "    random_forest_tune_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicting the Classifier on the Validation Data\n",
    "    y_validate_tune_pred = random_forest_tune_model.predict(X_validate)\n",
    "    \n",
    "    # Calculating the Accuracy\n",
    "    churn_tune_accuracy = round((accuracy_score(y_validate, y_validate_tune_pred))*100, 2)\n",
    "    \n",
    "    # Incrementing the Scenario_ID for Tracking\n",
    "    scenario_id += 1\n",
    "    \n",
    "    # Displaying the Accuracy Metrics for the Various Combinations of the Hyper-Parameters Tuning\n",
    "    print(\" Scenario {} - max_depth: {}, max_features: {}, Classification Accuracy: {}%\".format(scenario_id, p1, p2, churn_tune_accuracy))\n",
    "    \n",
    "    # Defining the Instance of Confusion Matrix\n",
    "    plot_confusion_matrix(random_forest_tune_model, X_validate, y_validate)  \n",
    "    plt.show()\n",
    "    title = \"Confusion Matrix {} - max_depth: {}, max_features: {}, Classification Accuracy: {}%\".format(scenario_id, p1, p2, churn_tune_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b2750-b928-4e8b-8930-dceadb9624b6",
   "metadata": {},
   "source": [
    "### As we can see from the above results; the Random Forest Classifier Model tuned with the parameters as max_depth = 6 and max_features as auto, sqrt, log2 has performed better in the validation data with the accuracy of about 84.5%.\n",
    "\n",
    "### Hence all of these 3 models can be considered as the Optimized Models for further deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce0038-e519-4fbe-ad5e-9412de6b514b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
